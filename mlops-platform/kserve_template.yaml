---
apiVersion: v1
kind: Secret
metadata:
  name: s3creds
  namespace: default # Change this to a designated namespace
  annotations:
     serving.kserve.io/s3-endpoint: ${S3_HOST} # This is an environment variable
     serving.kserve.io/s3-usehttps: "1"
     serving.kserve.io/s3-region: ${S3_REGION_NAME} # This is an environment variable
     serving.kserve.io/s3-useanoncredential: "false"
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
  AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa
  namespace: default # Change this to a designated namespace
  annotations:
     serving.kserve.io/s3-endpoint: ${S3_HOST}
     serving.kserve.io/s3-usehttps: "1"
     serving.kserve.io/s3-region: ${S3_REGION_NAME}
     serving.kserve.io/s3-useanoncredential: "false"
secrets:
  - name: s3creds
---
apiVersion: "serving.kserve.io/v1beta1"
kind: InferenceService
metadata:
  name: iris-classifier
  namespace: default  # Change this to a designated namespace
spec:
  predictor:
    containerConcurrency: 10 # Ten requests per pod (InferenceService replica)
    minReplicas: 1 # This prevents cold start
    maxReplicas: 3 # Maximum number of pods (surplus requests will be buffered)
    serviceAccountName: sa
    model:
      modelFormat:
        name: mlflow
      protocolVersion: v2
      storageUri: s3://${S3_BUCKET_NAME}/${MLFLOW_ROOT_PREFIX}/${MLFLOW_EXPERIMENT_ID}/${MLFLOW_RUN_ID}/artifacts/estimator
      resources: # Resources for each replica
        limits:
          cpu: "3" # You can change these via the endpoint configuration located in the artifact's s3 path
          memory: "5Gi"
        requests:
          cpu: "3" # Same here
          memory: "5Gi"
